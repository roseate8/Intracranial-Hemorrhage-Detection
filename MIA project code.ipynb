{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"MIA project code.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tZpKuHtZsrOv","colab_type":"text"},"source":["## Import dependencies\n","\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"El4dKc4PsgXL","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import pydicom\n","import os\n","import collections\n","import sys\n","import glob\n","import random\n","import cv2\n","import tensorflow as tf\n","import multiprocessing\n","\n","from math import ceil, floor\n","from copy import deepcopy\n","from tqdm import tqdm\n","from imgaug import augmenters as iaa\n","\n","import keras\n","import keras.backend as K\n","from keras.callbacks import Callback, ModelCheckpoint\n","from keras.layers import Dense, Flatten, Dropout\n","from keras.models import Model, load_model\n","from keras.utils import Sequence\n","from keras.losses import binary_crossentropy\n","from keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7IfriXc_stQy","colab_type":"text"},"source":["#### Install Multi-label stratification package and EfficientNet model with pretrained weights."]},{"cell_type":"code","metadata":{"trusted":true,"id":"yCUk_Yt6sgXW","colab_type":"code","colab":{}},"source":["# Install Modules from internet\n","!pip install efficientnet\n","!pip install iterative-stratification"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QdkyVMds9MR","colab_type":"text"},"source":["#### Input path to the data must be changed if ran locally"]},{"cell_type":"code","metadata":{"trusted":true,"id":"bM1VyI1AsgXc","colab_type":"code","colab":{}},"source":["from pathlib import Path\n","\n","input_path = Path(\"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection\")\n","%ls ../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"NdbPZN_0sgXj","colab_type":"code","colab":{}},"source":["# Import modules\n","import efficientnet.keras as efn \n","from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiHMVm60tMGg","colab_type":"text"},"source":["#### Data directory must be changed if ran locally"]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"618wc8M0sgXr","colab_type":"code","colab":{}},"source":["# Seed\n","SEED = 12345\n","np.random.seed(SEED)\n","tf.set_random_seed(SEED)\n","\n","# Constants\n","NEW_DATASET_SIZE = 0.1  # Size of the shrunk dataset\n","HEIGHT = 256\n","WIDTH = 256\n","CHANNELS = 3\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 64\n","SHAPE = (HEIGHT, WIDTH, CHANNELS)\n","\n","# Folders\n","DATA_DIR = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\n","TEST_IMAGES_DIR = DATA_DIR + 'stage_2_test/'\n","TRAIN_IMAGES_DIR = DATA_DIR + 'stage_2_train/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhYpi7G2tRd9","colab_type":"text"},"source":["#### Windowing Operation (Brain, Subdural and Soft tissue windows)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"itsfWayjsgXy","colab_type":"code","colab":{}},"source":["def correct_dcm(dcm):\n","    x = dcm.pixel_array + 1000\n","    px_mode = 4096\n","    x[x>=px_mode] = x[x>=px_mode] - px_mode\n","    dcm.PixelData = x.tobytes()\n","    dcm.RescaleIntercept = -1000\n","\n","def window_image(dcm, window_center, window_width): \n","    ''' Given an dicom image, window level (WL) and window width (WW), the function calculates the upper\n","    and lower grey levels. Voxel values above the upper grey level will become white and below the lower\n","    grey level will become black. '''   \n","\n","    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n","        correct_dcm(dcm)\n","    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n","    \n","    # Resize\n","    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n","\n","    # lower grey level (y)\n","    img_min = window_center - window_width // 2\n","    # upper grey level (x)\n","    img_max = window_center + window_width // 2\n","    # Highlight the voxels above x and darken the voxels below y\n","    img = np.clip(img, img_min, img_max)\n","    return img\n","\n","def bsb_window(dcm):\n","    ''' Create the three types of windows and stack them as the RGB channels '''\n","    # Brain matter window\n","    brain_img = window_image(dcm, 40, 80)\n","    # Blood/subdural window\n","    subdural_img = window_image(dcm, 80, 200)\n","    # Soft tissue window\n","    soft_img = window_image(dcm, 40, 380)\n","    \n","    brain_img = (brain_img - 0) / 80\n","    subdural_img = (subdural_img - (-20)) / 200\n","    soft_img = (soft_img - (-150)) / 380\n","\n","    # Stack the windows as RGB channels.\n","    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n","    return bsb_img\n","\n","def _read(path, SHAPE):\n","    dcm = pydicom.dcmread(path)\n","    try:\n","        img = bsb_window(dcm)\n","    except:\n","        img = np.zeros(SHAPE)\n","    return img"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_NSmCNhvtaBZ","colab_type":"text"},"source":["#### Train Data generator (real-time augmentation and windows as channels)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"jiLZANPdsgX4","colab_type":"code","colab":{}},"source":["# Image Augmentation\n","sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n","augmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n","                                iaa.Flipud(0.10),\n","                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n","                            ], random_order = True)       \n","        \n","# Generators\n","class TrainDataGenerator(keras.utils.Sequence):\n","    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n","        self.dataset = dataset\n","        self.ids = dataset.index\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.img_size = img_size\n","        self.img_dir = img_dir\n","        self.augment = augment\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return int(ceil(len(self.ids) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n","        X, Y = self.__data_generation(indices)\n","        return X, Y\n","\n","    def augmentor(self, image):\n","        augment_img = augmentation        \n","        image_aug = augment_img.augment_image(image)\n","        return image_aug\n","\n","    def on_epoch_end(self):\n","        self.indices = np.arange(len(self.ids))\n","        np.random.shuffle(self.indices)\n","\n","    def __data_generation(self, indices):\n","        X = np.empty((self.batch_size, *self.img_size))\n","        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n","        \n","        for i, index in enumerate(indices):\n","            ID = self.ids[index]\n","            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n","            if self.augment:\n","                X[i,] = self.augmentor(image)\n","            else:\n","                X[i,] = image\n","            Y[i,] = self.labels.iloc[index].values        \n","        return X, Y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-gF2c13ti1Q","colab_type":"text"},"source":["#### Load the training set; Perform data cleaning"]},{"cell_type":"code","metadata":{"trusted":true,"id":"lvtTS81BsgX7","colab_type":"code","colab":{}},"source":["def read_trainset(filename = DATA_DIR + \"stage_2_train.csv\"):\n","    df = pd.read_csv(filename)\n","    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n","    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n","\n","    duplicates_to_remove = df[df[\"ID\"].duplicated(keep=False)].index\n","    \n","    df = df.drop(index = duplicates_to_remove)\n","    \n","    df = df.reset_index(drop = True)    \n","    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n","    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n","    return df\n","\n","# Read Train Dataset\n","train_df = read_trainset()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4rJcfDwtluj","colab_type":"text"},"source":["#### Exploration on the training set"]},{"cell_type":"code","metadata":{"trusted":true,"id":"8nLbz4xdsgYB","colab_type":"code","colab":{}},"source":["train_df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"N8MaDjuhsgYH","colab_type":"code","colab":{}},"source":["print('Total number of samples:', len(train_df))\n","print('Number of samples where a tumour exists:', sum(train_df.sum(axis = 1)>0))\n","assert train_df.Label['any'].sum() == sum(train_df.sum(axis = 1)>0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"bDiqtYWisgYM","colab_type":"code","colab":{}},"source":["# Only 15% of samples have a tumour\n","tot_rows = len(train_df)\n","print('Percentage of positive samples in train set:')\n","for col in train_df['Label'].columns:\n","    print(f'{col}: {round(100*sum(train_df.Label[col])/tot_rows, 2)}')\n","    \n","# epidural has very few samples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KS-1hKgZtokN","colab_type":"text"},"source":["#### Oversample the epidural cases three times, i.e., 8x "]},{"cell_type":"code","metadata":{"trusted":true,"id":"j7mUuxr5sgYR","colab_type":"code","colab":{}},"source":["# Oversample the epidural cases three times, i.e., 8x \n","for i in range(3):\n","    epidural_df = train_df[train_df.Label['epidural'] == 1]\n","    train_oversample_df = pd.concat([train_df, epidural_df])\n","    train_df = train_oversample_df\n","\n","# Summary\n","print('Train Shape: {}'.format(train_df.shape))\n","print('Test Shape: {}'.format(test_df.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"7DPAxemUsgYU","colab_type":"code","colab":{}},"source":["print('Percentage of positive samples in train set:')\n","for col in train_df['Label'].columns:\n","    print(f'{col}: {round(100*sum(train_df.Label[col])/tot_rows, 2)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0Hr0LVWttHu","colab_type":"text"},"source":["#### Model Selection. User should uncomment the model they wish to run."]},{"cell_type":"code","metadata":{"trusted":true,"id":"AazHsJ8jsgYZ","colab_type":"code","colab":{}},"source":["# Model checkpoint to save model weights\n","def ModelCheckpointFull(model_name):\n","    return ModelCheckpoint(model_name, \n","                            monitor = 'val_acc', \n","                            verbose = 1, \n","                            save_best_only = True, \n","                            save_weights_only = True, \n","                            mode = 'max', \n","                            period = 1)\n","\n","# Model selection\n","def create_model():\n","    from keras.applications import resnet_v2, resnet, vgg16, inception_v3, mobilenet_v2, xception\n","    K.clear_session()\n","    \n","#     base_model =  efn.EfficientNetB3(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n","#     base_model = efn.EfficientNetB0(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE) # Commit V2\n","#     base_model =  resnet.ResNet50(include_top=False, weights='imagenet', input_shape=SHAPE, pooling='avg')\n","#     base_model = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=SHAPE, pooling='avg')\n","#     base_model = inception_v3.InceptionV3(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n","#     base_model =  efn.EfficientNetB5(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n","#     base_model =  mobilenet_v2.MobileNetV2(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n","#     base_model = xception.Xception(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n","    \n","    x = base_model.output\n","    y_pred = Dense(6, activation = 'sigmoid')(x)\n","\n","    return Model(inputs = base_model.input, outputs = y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbF4cjFAsgYc","colab_type":"text"},"source":["#### Reduce dataset size to 10% of processed dataset"]},{"cell_type":"code","metadata":{"trusted":true,"id":"wnueqVHFsgYd","colab_type":"code","colab":{}},"source":["# Multi Label Stratified Split stuff...\n","msss = MultilabelStratifiedShuffleSplit(n_splits = 1, test_size = NEW_DATASET_SIZE, random_state = SEED)\n","X = train_df.index\n","Y = train_df.Label.values\n","\n","# Get train and test index\n","msss_splits = next(msss.split(X, Y))\n","new_dataset_idx = msss_splits[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nyw-1ZSAuIr4","colab_type":"text"},"source":["#### Perform train-validation split"]},{"cell_type":"code","metadata":{"trusted":true,"id":"lHXbpOmtsgYh","colab_type":"code","colab":{}},"source":["# Perform train-val 85-15 stratified split on multilabel\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits = 1, test_size = 0.15, random_state = SEED)\n","X = train_df.iloc[new_dataset_idx].index\n","Y = train_df.iloc[new_dataset_idx].Label.values\n","\n","# Get train and valid index\n","msss_splits = next(msss.split(X, Y))\n","train_idx = msss_splits[0]\n","valid_idx = msss_splits[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"KHRemVSfsgYk","colab_type":"code","colab":{}},"source":["print('Size of training set: ', len(train_idx))\n","print('Size of validation set:', len(valid_idx))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9rNS8x9uNwW","colab_type":"text"},"source":["#### Define the model training and execute"]},{"cell_type":"code","metadata":{"trusted":true,"id":"XjP4plkAsgYq","colab_type":"code","colab":{}},"source":["# Create Data Generators for Train and Valid\n","data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n","                                            train_df.iloc[train_idx], \n","                                            TRAIN_BATCH_SIZE, \n","                                            SHAPE,\n","                                            augment = True)\n","\n","data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n","                                        train_df.iloc[valid_idx], \n","                                        VALID_BATCH_SIZE, \n","                                        SHAPE,\n","                                        augment = False)\n","\n","# Create Model\n","model = create_model()\n","\n","# Full Training Model\n","for base_layer in model.layers[:-1]:\n","    base_layer.trainable = True\n","\n","LR = 0.0001\n","\n","model.compile(optimizer = Adam(learning_rate = LR), \n","              loss = 'binary_crossentropy',\n","              metrics = ['acc', tf.keras.metrics.AUC()])\n","\n","# Train Model\n","hist = model.fit_generator(generator = data_generator_train,\n","                    validation_data = data_generator_val,\n","                    epochs = 10,\n","                    callbacks = [ModelCheckpointFull('effb3.h5')],\n","                    verbose = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AH3cviRuRdv","colab_type":"text"},"source":["#### Plot the accuracy and loss curves"]},{"cell_type":"code","metadata":{"trusted":true,"id":"hgIqMwCbsgYu","colab_type":"code","colab":{}},"source":["history = hist\n","import matplotlib.pyplot as plt\n","\n","\n","fig = plt.figure(figsize = (8, 6))\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Accuracy curve', fontsize = 18)\n","plt.ylabel('accuracy', fontsize = 16)\n","plt.xlabel('epoch', fontsize = 16)\n","plt.legend(['train', 'val'], loc='upper left')\n","# plt.show()\n","fig.savefig('acc.png', bbox_inches = 'tight')\n","\n","fig = plt.figure(figsize = (8, 6))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Loss curve',fontsize = 18)\n","plt.ylabel('loss', fontsize = 16)\n","plt.xlabel('epoch', fontsize = 16)\n","plt.legend(['train', 'val'], loc='upper left')\n","# plt.show()\n","fig.savefig('loss.png', bbox_inches = 'tight')"],"execution_count":0,"outputs":[]}]}